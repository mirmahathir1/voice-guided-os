# Voice-Guided OS (Text-Guided Desktop Controller)

A Python application that uses text commands to control your desktop through an LLM-powered visual understanding system. The system analyzes screenshots, determines actions, and executes precise mouse clicks using a progressive grid-based refinement approach.

## ğŸ¯ Overview

This project enables you to control your macOS desktop using natural language commands. Simply describe what you want to do (e.g., "Open Chrome", "Click the submit button", "Navigate to settings"), and the system will:

1. Capture a screenshot of your screen
2. Use GPT-4 Vision to analyze the screenshot and determine the next action
3. Use a two-stage grid system (10x10 â†’ 2x2) to precisely locate click targets
4. Execute the click and repeat until the task is complete

## ğŸ—ï¸ How It Works

### Architecture

The system follows a modular architecture with the following key components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MAIN CONTROL LOOP                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Capture Screenshot                                      â”‚
â”‚     â†“                                                        â”‚
â”‚  2. Send to LLM with command â†’ Get Action Plan              â”‚
â”‚     â†“                                                        â”‚
â”‚  3. If action requires click:                               â”‚
â”‚     a. Overlay 10x10 grid â†’ LLM selects cell               â”‚
â”‚     b. Crop cell, overlay 2x2 grid â†’ LLM refines selection â”‚
â”‚     c. Calculate center point and click                    â”‚
â”‚     â†“                                                        â”‚
â”‚  4. Repeat until COMPLETE/ERROR                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Two-Stage Grid Refinement

The system uses a progressive refinement approach for precise click targeting:

1. **10x10 Grid Selection**: The full screen is divided into a 10x10 grid (columns 1-10, rows A-J). The LLM selects the cell containing the target element.

2. **2x2 Refinement**: The selected cell is cropped and divided into a 2x2 grid (columns 1-2, rows A-B). The LLM refines the selection to pinpoint the exact click location.

3. **Click Execution**: The center of the final 2x2 cell is calculated and the click is executed.

This approach balances precision with efficiency, allowing the LLM to make coarse selections first, then refine to pixel-accurate clicks.

### Action Loop

The main control loop runs iteratively:

- Each iteration captures a new screenshot to see the current state
- The LLM analyzes the screenshot and determines the next action
- Actions can be:
  - `MOUSE_LEFT_CLICK`: Click at a specific location (requires grid selection)
  - `COMPLETE`: Task is finished
  - `ERROR`: Task cannot be completed (with reason)
- The loop continues until COMPLETE or ERROR, or until max iterations (20) is reached

## ğŸ“‹ Requirements

- **OS**: macOS (primary monitor only)
- **Python**: 3.8+ (uses local conda environment in `./env/`)
- **Permissions**: Screen recording and accessibility permissions must be granted
- **API Key**: OpenAI API key with access to GPT-4 Vision models

## ğŸš€ Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd voice-guided-os
   ```

2. **Set up Python environment**:
   ```bash
   # Create conda environment (or use your preferred method)
   conda create -n voice-guided-os python=3.8
   conda activate voice-guided-os
   
   # Install dependencies
   pip install -r requirements.txt
   ```

3. **Configure API key**:
   Create a `.env` file in the project root:
   ```bash
   OPENAI_API_KEY=your_api_key_here
   ```

4. **Grant permissions**:
   - System Preferences â†’ Security & Privacy â†’ Privacy
   - Enable "Screen Recording" for Terminal/Python
   - Enable "Accessibility" for Terminal/Python

## ğŸ’» Usage

Run the application:

```bash
python main.py
```

The application will start in terminal mode. Enter your commands at the prompt:

```
Enter command: Open Chrome
Enter command: Click the search bar
Enter command: Navigate to settings
```

### Command Examples

- `"Open Chrome"` - Opens the Chrome application
- `"Click the submit button"` - Clicks a submit button on the screen
- `"Navigate to the settings menu"` - Finds and clicks settings
- `"Close the dialog"` - Closes an open dialog window
- `"Click on the first search result"` - Clicks the first result in a search

The system will:
1. Capture a screenshot
2. Analyze it with the LLM
3. Execute the necessary clicks
4. Repeat until the task is complete

Press `Ctrl+C` to exit.

## ğŸ“ Project Structure

```
voice-guided-os/
â”œâ”€â”€ main.py                     # Entry point and main control loop
â”œâ”€â”€ config.py                   # Configuration and environment variables
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables (API keys)
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ screenshot.py          # Screenshot capture (mss-based)
â”‚   â”œâ”€â”€ llm_client.py            # GPT-4 Vision API communication
â”‚   â”œâ”€â”€ action_executor.py       # Mouse click execution
â”‚   â”œâ”€â”€ grid_processor.py       # Grid overlay and coordinate calculation
â”‚   â””â”€â”€ execution_logger.py      # Execution logging and PDF generation
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ action_prompt.txt        # Prompt for action determination
â”‚   â”œâ”€â”€ grid_selection.txt       # Prompt for 10x10 grid selection
â”‚   â””â”€â”€ refinement_prompt.txt    # Prompt for 2x2 refinement
â”‚
â””â”€â”€ execution/                  # Auto-generated execution logs
    â””â”€â”€ <timestamp>/             # One folder per command run
        â”œâ”€â”€ command.txt          # Original user command
        â”œâ”€â”€ step_01_complete.pdf # Complete documentation for step 1
        â”œâ”€â”€ step_02_complete.pdf # Complete documentation for step 2
        â””â”€â”€ ...
```

## ğŸ”§ Configuration

Key settings in `config.py`:

- **LLM Model**: `gpt-4o` (GPT-4 Vision)
- **Grid Size**: 10x10 main grid, 2x2 refinement grid
- **Max Iterations**: 20 (prevents infinite loops)
- **Action Delay**: 1.0 second (wait time after each click)
- **Click Marker**: Red circle with 15px radius

## ğŸ“Š Execution Logging

Every command execution is logged with complete documentation:

### Execution Folders

Each command run creates a timestamped folder in `execution/`:
```
execution/
â”œâ”€â”€ 2026-01-11_14-32-05/
â”‚   â”œâ”€â”€ command.txt
â”‚   â”œâ”€â”€ step_01_complete.pdf
â”‚   â”œâ”€â”€ step_02_complete.pdf
â”‚   â””â”€â”€ ...
```

### PDF Documentation

Each step generates a comprehensive PDF containing:
- **Main Prompt**: The prompt sent to the LLM for action determination
- **Screenshot**: The screen state before the action
- **Main Response**: The LLM's action plan
- **Grid Selection**: 10x10 grid overlay and selection
- **Refinement Selection**: 2x2 refinement grid and selection
- **Final Screenshot**: Screen state after click (with red click marker)

This logging system enables:
- **Debugging**: See exactly what the LLM saw and decided
- **Auditing**: Track all actions taken during execution
- **Improvement**: Analyze failures and refine prompts

## ğŸ§  LLM Integration

The system uses OpenAI's GPT-4 Vision API with three types of prompts:

1. **Action Determination**: Analyzes the screenshot and user command to determine the next action
2. **Grid Selection**: Selects a cell from the 10x10 grid containing the target element
3. **Refinement**: Selects a cell from the 2x2 grid for precise click location

### Conversation History

The LLM maintains conversation history within a single command execution, allowing it to:
- Remember previous actions taken
- Understand multi-step workflows
- Handle context-dependent commands

### Error Handling

- **Invalid JSON**: Retries with stricter format instructions
- **API Timeouts**: Exponential backoff retry (up to 3 attempts)
- **Invalid Coordinates**: Validates grid selections and raises errors
- **Max Iterations**: Stops after 20 iterations to prevent infinite loops

## ğŸ¯ Key Features

- **Visual Understanding**: Uses GPT-4 Vision to understand screen content
- **Precise Clicking**: Two-stage grid refinement for accurate targeting
- **Multi-Step Tasks**: Handles complex workflows automatically
- **Complete Logging**: PDF documentation for every step
- **Error Recovery**: Handles failures gracefully with retry logic
- **Context Awareness**: Maintains conversation history for context

## âš ï¸ Limitations

- **Left-Click Only**: Currently supports only left mouse clicks
- **macOS Only**: Designed for macOS (primary monitor)
- **Single Monitor**: Only captures primary display
- **No Keyboard Input**: Cannot type text or use keyboard shortcuts
- **No Right-Click**: Context menus not supported
- **No Drag-and-Drop**: Cannot drag elements

## ğŸš€ Future Plans

The current implementation is limited to **left-click only** for simplicity and to establish a solid foundation. Future enhancements will include:

### Additional Mouse Actions
- **Double-click**: For opening files, selecting words, etc.
- **Right-click**: For context menus
- **Drag-and-drop**: For moving files, rearranging items, selecting text ranges
- **Hover**: For tooltips and hover-activated menus

### Keyboard Actions
- **Text Input**: Type text into input fields and text areas
- **Keyboard Shortcuts**: Support for system shortcuts (Cmd+C, Ctrl+V, Cmd+Tab, etc.)
- **Special Keys**: Enter, Escape, Tab, Arrow keys, etc.

### Advanced Interactions
- **Scrolling**: Vertical and horizontal scrolling to access off-screen elements. This will enable handling of elements that require scrolling to become visible.
- **Multi-step Sequences**: Complex workflows combining multiple action types
- **Action Chaining**: Automatic sequencing (e.g., click field â†’ type text â†’ press Enter)
- **Multi-Application Commands**: Support for commands that span multiple applications (e.g., "Copy text from Safari and paste it in Notes"). Application switching will be handled by clicking on application icons/windows. Currently limited to left-click only.

### Implementation Considerations for Future Features
- **Action Type Detection**: LLM will need to determine the appropriate action type based on context
- **Focus Management**: For typing, ensure the correct field is focused before input
- **Timing and Delays**: Handle variable response times for different action types
- **Error Recovery**: Retry mechanisms for failed actions with alternative approaches
- **Small Target Precision**: For very small UI elements (checkboxes, close buttons), consider whether a third refinement level beyond the 2x2 grid is needed for sufficient precision

## ğŸ› Troubleshooting

### API Key Issues
- Ensure `.env` file exists with `OPENAI_API_KEY` set
- Verify the API key has access to GPT-4 Vision models

### Permission Issues
- Grant Screen Recording permission in System Preferences
- Grant Accessibility permission in System Preferences

### Click Accuracy Issues
- Check that the target element is visible in screenshots
- Review execution logs to see what the LLM selected
- Adjust grid sizes in `config.py` if needed

### Infinite Loops
- The system stops after 20 iterations automatically
- Check execution logs to see why the loop didn't complete
- Verify the task can be completed with left-clicks only

